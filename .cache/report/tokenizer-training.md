## Tokenizer training
timestamp: 2025-12-21 16:00:24

- max_chars: 10,000,000,000
- doc_cap: 100,000
- vocab_size: 65,536
- train_time: 231.8372
- num_special_tokens: 9
- token_bytes_min: 1
- token_bytes_max: 64
- token_bytes_mean: 6.6288
- token_bytes_std: 2.7797


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19db082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture Overview:\n",
    "1. Embedding: Token IDs -> Vectors (wte)\n",
    "2. Stack of Blocks (Repeated L times):\n",
    "   - RMSNorm\n",
    "   - Attention (Mixing info between tokens)\n",
    "   - RMSNorm\n",
    "   - MLP (Processing info within a token)\n",
    "3. Final Norm \n",
    "4. LMHead: Vectors -> Logits (Probabilities)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Hyperparameters for the model.\n",
    "    \"\"\"\n",
    "    # ┌─────────────────────────────────────────────────────────┐\n",
    "    # │           321M CONVERSATIONAL MODEL                     │\n",
    "    # ├─────────────────────────────────────────────────────────┤\n",
    "    # │  hidden_dim:        1024                                │\n",
    "    # │  layers:            24                                  │\n",
    "    # │  heads:             8                                   │\n",
    "    # │  head_dim:          128                                 │\n",
    "    # │  mlp_ratio:         3x                                  │\n",
    "    # │  vocab_size:        32K                                 │\n",
    "    # │  context_length:    1024                                │\n",
    "    # │  embedding:         tied (input = output projection)    │\n",
    "    # │  activation:        relu squared                        │\n",
    "    # │  position encoding: RoPE                                │\n",
    "    # ├─────────────────────────────────────────────────────────┤\n",
    "    # │  TOTAL PARAMETERS:  243,269,632                         │\n",
    "    # └─────────────────────────────────────────────────────────┘\n",
    "    # No KV cache\n",
    "    # No GQA\n",
    "\n",
    "    hidden_dim: int = 1024 # hidden dimension\n",
    "    n_layers: int = 24 # May need to reduce to 22 or 20\n",
    "    n_heads: int = 8 # head dimension = hidden_dim / n_heads = 128\n",
    "    mlp_ratio: int = 3\n",
    "    vocab_size: int = 32*1024\n",
    "    sequence_len: int = 1024\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    RMSNorm (Root Mean Square Layer Normalization).\n",
    "    Used to stabilize training by normalizing activation magnitudes.\n",
    "    \"\"\"\n",
    "    # Purely functional rmsnorm with no learnable params\n",
    "    return F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "\n",
    "def apply_rotatory_positional_encoding(x, cos, sin):\n",
    "    \"\"\"\n",
    "    Applies Rotary Positional Embeddings (RoPE).\n",
    "    Rotates the query and key vectors to encode relative positions.\n",
    "    \"\"\"\n",
    "    assert x.ndim == 4  # multihead attention\n",
    "    d = x.shape[3] // 2\n",
    "    x1, x2 = x[..., :d], x[..., d:] # split up last time into two halves\n",
    "    y1 = x1 * cos + x2 * sin # rotate pairs of dims\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    out = torch.cat([y1, y2], 3) # re-assemble\n",
    "    out = out.to(x.dtype) # ensure input/output dtypes match\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c550322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(hidden_dim=1024, n_layers=24, n_heads=8, mlp_ratio=3, vocab_size=32768, sequence_len=1024)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2883e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self Attention.\n",
    "    \n",
    "    1. Projects input to Q, K, V.\n",
    "    2. Applies RoPE to Q, K for position info.\n",
    "    3. Computes attention scores (Q @ K) to see how much each token cares about others. Aggregates values (V) based on scores.\n",
    "    4. Projects output to mix information across heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.head_dim = config.hidden_dim // config.n_heads\n",
    "\n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.key = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.query = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.value = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "\n",
    "        # Output projection (\"o\"): mixes results from all heads back into n_embd\n",
    "        self.proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cos_sin: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 1. Projects input to Q, K, V.\n",
    "        # reshape to (B, T, n_heads, head_dim)\n",
    "        k = self.key(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        q = self.query(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.value(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # 2. Applies RoPE to Q, K for position info.\n",
    "        cos, sin = cos_sin\n",
    "        k, q = apply_rotatory_positional_encoding(k, cos, sin), apply_rotatory_positional_encoding(q, cos, sin)\n",
    "\n",
    "        # 3. Computes attention scores (Q @ K) to see how much each token cares about others.\n",
    "        q, k = norm(q), norm(k) # QK norm\n",
    "\n",
    "        # make head be batch dim, i.e. (B, T, n_heads, head_dim) -> (B, n_heads, T, head_dim)\n",
    "        # We are making the n_heads into a batch dimension so pytorch treats it as batches and\n",
    "        # applies the attention function on each head separately in parallel\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)         \n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # Re-assemble the heads side by side and project back to residual stream\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 4. Projects output to mix information across heads.\n",
    "        y = self.proj(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21bcf98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = MultiHeadAttention(config)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ed627ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param in attn.parameters():\n",
    "    print(type(param), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe7e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# %pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b48b87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1830\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: MultiHeadAttention.forward() missing 1 required positional argument: 'cos_sin'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2171597071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(attn, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc2fc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Network (MLP).\n",
    "    Processes each token independently (no mixing between tokens).\n",
    "    Structure: Expand -> ReLU^2 -> Contract\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.proj_up = nn.Linear(config.hidden_dim, config.hidden_dim * config.mlp_ratio, bias=False)\n",
    "        self.proj_down = nn.Linear(config.hidden_dim * config.mlp_ratio, config.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj_up(x)\n",
    "        x = F.relu(x).square()\n",
    "        # TODO: Check if swiglu is better for 3 x hidden_dim - \n",
    "        # gelu and silu are alternatives but difference seems marginal so sticking with relu^2\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bebf83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (proj_up): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "  (proj_down): Linear(in_features=3072, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(config)\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FeedForward                              [1, 1024, 1024]           --\n",
       "├─Linear: 1-1                            [1, 1024, 3072]           3,145,728\n",
       "├─Linear: 1-2                            [1, 1024, 1024]           3,145,728\n",
       "==========================================================================================\n",
       "Total params: 6,291,456\n",
       "Trainable params: 6,291,456\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.29\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 33.55\n",
       "Params size (MB): 25.17\n",
       "Estimated Total Size (MB): 62.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ff, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4eed103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Block.\n",
    "    Contains:\n",
    "    1. Attention (Communication)\n",
    "    2. MLP (Computation)\n",
    "    Both use Residual Connections (x + ...) and Pre-Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ff = FeedForward(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, cos_sin: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention with residual connection\n",
    "        x = x + self.attn(norm(x), cos_sin)\n",
    "        # MLP with residual connection\n",
    "        x = x + self.ff(norm(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c4ae3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (attn): MultiHeadAttention(\n",
       "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (proj_up): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "    (proj_down): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerBlock(GPTConfig())\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe299056",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1759942446.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "summary(block, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "694a4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    The full GPT model.\n",
    "    Contains:\n",
    "    1. Token Embedding\n",
    "    2. Transformer Blocks (stacked)\n",
    "    3. Final Normalization\n",
    "    4. LM Head - Tied weights with token embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "\n",
    "        self.rotary_seq_len = config.sequence_len * 10 # 10X over-compute\n",
    "        # Why 10x? This provides a generous buffer for inference/generation, allowing the model\n",
    "        # to generate sequences longer than its training length without recomputing embeddings.\n",
    "        # Note: While the embeddings support 10x length, the model's quality degrades beyond ~1.5-2x\n",
    "        # the training length due to unseen attention patterns. This buffer is for convenience,\n",
    "        # not an expectation of good performance at 10x length. Memory cost is negligible.\n",
    "        \n",
    "        head_dim = config.hidden_dim // config.n_heads\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False) # persistent=False means it's not saved to the checkpoint\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "    \n",
    "    def forward(self, idx, targets=None, loss_reduction=\"mean\") -> torch.Tensor:\n",
    "        T = idx.shape[1]\n",
    "        cos_sin = self.cos[:, :T], self.sin[:, :T] # truncate cache to current sequence length\n",
    "        x = self.token_embedding(idx)\n",
    "        x = norm(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, cos_sin)\n",
    "        x = norm(x)\n",
    "\n",
    "        softcap = 15 # smoothly cap the logits to the range [-softcap, softcap]\n",
    "        logits = self.lm_head(x)\n",
    "        logits = logits.float() # switch to fp32 for logit softcap and loss computation\n",
    "        logits = softcap * torch.tanh(logits / softcap) # squash the logits\n",
    "        \n",
    "        if targets is not None:\n",
    "            # training: given the targets, compute and return the loss\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\n",
    "            return loss\n",
    "        else:\n",
    "            # inference: just return the logits directly\n",
    "            return logits\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Custom weight initialization scheme.\n",
    "        \n",
    "        The initialization logic deviates from PyTorch defaults (Kaiming defaults) to improve training \n",
    "        stability and convergence for deep Transformers.\n",
    "        \n",
    "        Key Difference:\n",
    "        Zero Initialization for Output Projections:\n",
    "        - Function: Sets the weights of the final linear layer in each block to zero.\n",
    "        - Why: This ensures that at initialization, the residual blocks contribute nothing to the \n",
    "            residual stream (y = x + 0). The model effectively starts as an identity function, allowing \n",
    "            unimpeded gradient flow from top to bottom. This prevents vanishing/exploding gradients \n",
    "            and provides a stable starting point for the model to gradually learn features.\n",
    "        \"\"\"\n",
    "        # Initialize the weights of the model\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Zero out the output projections of the blocks\n",
    "        for block in self.blocks:\n",
    "            torch.nn.init.zeros_(block.ff.proj_down.weight)\n",
    "            torch.nn.init.zeros_(block.attn.proj.weight)\n",
    "\n",
    "        # init the rotary embeddings\n",
    "        head_dim = self.config.hidden_dim // self.config.n_heads\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.cos, self.sin = cos, sin\n",
    "\n",
    "        # Cast the embeddings from fp32 to bf16: optim can tolerate it and it saves memory: both in the model and the activations\n",
    "        if self.token_embedding.weight.device.type == \"cuda\":\n",
    "            self.token_embedding.to(dtype=torch.bfloat16)\n",
    "\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Custom initialization for Linear and Embedding layers.\n",
    "        \n",
    "        1. Controlled Variance (Linear Layers):\n",
    "           - Formula: std = 1 / sqrt(fan_in) * min(1, sqrt(fan_out / fan_in))\n",
    "           - Why: Standard Kaiming init often leads to activation variance that grows with depth in \n",
    "             Transformers. This custom initialization (ref: https://arxiv.org/pdf/2310.17813) stabilizes \n",
    "             activation variance across layers, specifically accounting for the network width.\n",
    "\n",
    "        2. Unit Variance (Embeddings):\n",
    "           - Function: Normal distribution with std=1.0.\n",
    "           - Why: Ensures strong initial signal strength before it enters the first normalization layer.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # https://arxiv.org/pdf/2310.17813 \n",
    "            fan_out = module.weight.size(0)\n",
    "            fan_in = module.weight.size(1)\n",
    "            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
    "\n",
    "\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n",
    "        # autodetect the device from model embeddings\n",
    "        if device is None:\n",
    "            device = self.token_embedding.weight.device\n",
    "        # stride the channels\n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "        # stride the time steps\n",
    "        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "        # calculate the rotation frequencies at each (time, channel) pair\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        cos, sin = freqs.cos(), freqs.sin()\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16() # keep them in bfloat16\n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :] # add batch and head dims for later broadcasting\n",
    "        return cos, sin\n",
    "\n",
    "    def setup_optimizers(self, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0):\n",
    "        \"\"\"\n",
    "        Sets up the optimizers.\n",
    "        Uses AdamW for embeddings/head and Muon for internal linear layers.\n",
    "\n",
    "        Detailed Explanation of Hybrid Strategy:\n",
    "        ----------------------------------------\n",
    "        We use two different optimizers because different parts of the Transformer have different \n",
    "        geometric properties and optimization landscapes.\n",
    "\n",
    "        1. Muon (for internal 2D matrices):\n",
    "           - Applied to: Attention projections (c_q, c_k, c_v, c_proj) and MLP weights (c_fc, c_proj).\n",
    "           - Mechanism: Muon forces weight *updates* to be orthogonal. In linear algebra, orthogonal \n",
    "             transformations (like rotation or reflection) preserve the magnitude (norm) of the vector \n",
    "             they act on.\n",
    "           - Benefit: Deep networks suffer from vanishing/exploding gradients because signals get \n",
    "             scaled up or down at every layer. By forcing updates to be orthogonal, Muon ensures \n",
    "             signals propagate through the network without exploding in magnitude, allowing for \n",
    "             much faster and more stable training of deep layers.\n",
    "\n",
    "        2. AdamW (for embeddings & head):\n",
    "           - Applied to: Token embeddings (wte) and the final output head (lm_head).\n",
    "           - Reason: These parameters are not dense 2D matrices in the same sense (embeddings are \n",
    "             lookup tables). The concept of \"orthogonal updates\" is mathematically ill-defined or \n",
    "             harmful for vectors/lookups. AdamW is ideal here as it adapts learning rates per-parameter \n",
    "             based on update frequency (handling the sparse nature of token updates).\n",
    "\n",
    "        Do they conflict? \n",
    "        No. Both optimizers step in directions derived from the same global loss gradient, so they \n",
    "        optimize the same function. The risk is learning speed mismatch (one part learning faster \n",
    "        than the other), which we handle by manually scaling the AdamW learning rate below.\n",
    "        \"\"\"\n",
    "        model_dim = self.config.hidden_dim\n",
    "        # ddp, rank, local_rank, world_size = get_dist_info()\n",
    "        # Separate out all parameters into 3 groups (matrix, embedding, lm_head)\n",
    "        matrix_params = list(self.blocks.parameters())\n",
    "        embedding_params = list(self.token_embedding.parameters())\n",
    "        assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params)\n",
    "        \n",
    "        # Create the AdamW optimizer for the embedding\n",
    "        # Scale the LR for the AdamW parameters by ∝1/√dmodel (having tuned the LRs for 768 dim model)\n",
    "        dmodel_lr_scale = (model_dim / 768) ** -0.5\n",
    "        # if rank == 0:\n",
    "        print(f\"Scaling the LR for the AdamW parameters ∝1/√({model_dim}/768) = {dmodel_lr_scale:.6f}\")\n",
    "        adam_groups = [\n",
    "            dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),\n",
    "        ]\n",
    "        adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)\n",
    "        AdamWFactory = partial(torch.optim.AdamW, fused=True)\n",
    "        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
    "        \n",
    "        # Create the Muon optimizer for the linear layers\n",
    "        muon_kwargs = dict(lr=matrix_lr, momentum=0.95)\n",
    "        muon_optimizer = Muon(matrix_params, **muon_kwargs)\n",
    "\n",
    "        # Combine the two optimizers into one list\n",
    "        optimizers = [adamw_optimizer, muon_optimizer]\n",
    "        for opt in optimizers:\n",
    "            for group in opt.param_groups:\n",
    "                group[\"initial_lr\"] = group[\"lr\"]\n",
    "        return optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6254fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afb5f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6f25595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GPT                                      [1, 1024, 32768]          --\n",
       "├─Embedding: 1-1                         [1, 1024, 1024]           33,554,432\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-1      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-2             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-2             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-3      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-4             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-3             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-5      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-6             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-4             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-7      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-8             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-5             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-9      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-10            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-6             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-11     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-12            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-7             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-13     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-14            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-8             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-15     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-16            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-9             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-17     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-18            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-10            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-19     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-20            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-11            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-21     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-22            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-12            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-23     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-24            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-13            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-25     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-26            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-14            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-27     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-28            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-15            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-29     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-30            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-16            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-31     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-32            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-17            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-33     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-34            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-18            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-35     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-36            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-19            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-37     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-38            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-20            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-39     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-40            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-21            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-41     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-42            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-22            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-43     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-44            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-23            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-45     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-46            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-24            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-47     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-48            [1, 1024, 1024]           6,291,456\n",
       "├─Linear: 1-3                            [1, 1024, 32768]          33,554,432\n",
       "==========================================================================================\n",
       "Total params: 318,767,104\n",
       "Trainable params: 318,767,104\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 318.77\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1887.44\n",
       "Params size (MB): 1275.07\n",
       "Estimated Total Size (MB): 3162.51\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(gpt, input_size=(1, config.sequence_len), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47fa1992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual unique parameters: 285,212,672\n"
     ]
    }
   ],
   "source": [
    "# The weight tying is working correctly — torchinfo just doesn't detect shared parameters by default. \n",
    "# It counts each layer's parameters independently.\n",
    "\n",
    "# This counts UNIQUE parameters (correct count with tying)\n",
    "real_params = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"Actual unique parameters: {real_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "934e412b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object: True\n",
      "Same memory: True\n"
     ]
    }
   ],
   "source": [
    "# These should all be True\n",
    "print(\"Same object:\", gpt.lm_head.weight is gpt.token_embedding.weight)\n",
    "print(\"Same memory:\", gpt.lm_head.weight.data_ptr() == gpt.token_embedding.weight.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01093d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e0192d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                               | Type            | Dtype\n",
      "----------------------------------------------------------------------\n",
      "token_embedding.weight                   | Parameter       | torch.float32\n",
      "blocks.0.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.0.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.0.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.0.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.0.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.0.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.1.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.1.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.1.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.1.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.1.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.1.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.2.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.2.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.2.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.2.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.2.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.2.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.3.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.3.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.3.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.3.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.3.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.3.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.4.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.4.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.4.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.4.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.4.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.4.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.5.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.5.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.5.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.5.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.5.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.5.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.6.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.6.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.6.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.6.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.6.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.6.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.7.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.7.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.7.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.7.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.7.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.7.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.8.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.8.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.8.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.8.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.8.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.8.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.9.attn.key.weight                 | Parameter       | torch.float32\n",
      "blocks.9.attn.query.weight               | Parameter       | torch.float32\n",
      "blocks.9.attn.value.weight               | Parameter       | torch.float32\n",
      "blocks.9.attn.proj.weight                | Parameter       | torch.float32\n",
      "blocks.9.ff.proj_up.weight               | Parameter       | torch.float32\n",
      "blocks.9.ff.proj_down.weight             | Parameter       | torch.float32\n",
      "blocks.10.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.10.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.10.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.10.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.10.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.10.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.11.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.11.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.11.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.11.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.11.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.11.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.12.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.12.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.12.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.12.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.12.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.12.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.13.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.13.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.13.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.13.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.13.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.13.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.14.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.14.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.14.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.14.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.14.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.14.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.15.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.15.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.15.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.15.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.15.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.15.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.16.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.16.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.16.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.16.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.16.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.16.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.17.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.17.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.17.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.17.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.17.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.17.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.18.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.18.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.18.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.18.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.18.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.18.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "blocks.19.attn.key.weight                | Parameter       | torch.float32\n",
      "blocks.19.attn.query.weight              | Parameter       | torch.float32\n",
      "blocks.19.attn.value.weight              | Parameter       | torch.float32\n",
      "blocks.19.attn.proj.weight               | Parameter       | torch.float32\n",
      "blocks.19.ff.proj_up.weight              | Parameter       | torch.float32\n",
      "blocks.19.ff.proj_down.weight            | Parameter       | torch.float32\n",
      "cos                                      | Buffer          | torch.bfloat16\n",
      "sin                                      | Buffer          | torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "def check_model_dtypes(model):\n",
    "    print(f\"{'Layer Name':<40} | {'Type':<15} | {'Dtype'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Check Parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name:<40} | Parameter       | {param.dtype}\")\n",
    "    \n",
    "    # Check Buffers (like RoPE cos/sin)\n",
    "    for name, buf in model.named_buffers():\n",
    "        print(f\"{name:<40} | Buffer          | {buf.dtype}\")\n",
    "check_model_dtypes(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "993fe345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample input for gpt model\n",
    "sample_input = torch.ones((1, 1), dtype=torch.int64)\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd36d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1, 32768])\n",
      "Max logit: 14.5836\n",
      "Predicted token ID: 1\n"
     ]
    }
   ],
   "source": [
    "gpt.eval()\n",
    "with torch.no_grad(): # Good practice for inference to save memory\n",
    "    op = gpt(sample_input)\n",
    "    \n",
    "print(f\"Output shape: {op.shape}\") # Should be (1, 1, vocab_size)\n",
    "print(f\"Max logit: {op.max().item():.4f}\")\n",
    "print(f\"Predicted token ID: {op.argmax().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766a77b",
   "metadata": {},
   "source": [
    "Predicted token is always same as the input value. Why?\n",
    "1. Weight Tying: We have set self.lm_head.weight = self.token_embedding.weight.\n",
    "2. Zero-Init Blocks: init_weights function sets the output projection of every Transformer block to zero.\n",
    " - This means the blocks (Attention and MLP) contribute nothing to the residual stream at initialization.\n",
    " - The model effectively acts as an identity function for the embeddings: Embedding(token) -> Norm -> Logits.\n",
    "3. Self-Similarity: Since the output head uses the same weights as the embedding, it calculates the dot product of the token's embedding vector with all other embedding vectors.\n",
    " - A vector's dot product with itself ($v \\cdot v$) is almost always much higher than with other random vectors ($v \\cdot w$).\n",
    " - Therefore, the model assigns the highest probability to the token that was input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b1f4c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss when taget = input: 0.025031551718711853\n",
      "loss when target != input: 14.135931968688965\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "sample_input2 = torch.ones((1, 1), dtype=torch.int64)*100\n",
    "print(f'loss when taget = input: {gpt(sample_input2, sample_input2)}')\n",
    "print(f'loss when target != input: {gpt(sample_input2, sample_input2*2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93cf34a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 285,212,672\n"
     ]
    }
   ],
   "source": [
    "orig_model = gpt # original, uncompiled model, for saving raw model state_dict and for inference/evaluation (because the shapes may change shape)\n",
    "# torch.compile: optimizing the model execution graph (JIT compilation)\n",
    "model = torch.compile(gpt, dynamic=False) # the inputs to model will never change shape so dynamic=False is safe\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc1aa760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate used by the internal SGD.\n",
    "        momentum: The momentum used by the internal SGD.\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        ns_steps: The number of Newton-Schulz iteration steps to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        params: list[Tensor] = [*params]\n",
    "        param_groups = []\n",
    "        for size in {p.numel() for p in params}:\n",
    "            group = dict(params=[p for p in params if p.numel() == size])\n",
    "            param_groups.append(group)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                assert g is not None\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf: Tensor = state[\"momentum_buffer\"]\n",
    "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
    "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "                p.add_(g, alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3600722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters ∝1/√(1024/768) = 0.866025\n"
     ]
    }
   ],
   "source": [
    "embedding_lr = 0.2\n",
    "weight_decay = 0.0\n",
    "matrix_lr = 0.02\n",
    "from functools import partial\n",
    "\n",
    "optimizers = model.setup_optimizers(embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb255ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67bd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19db082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture Overview:\n",
    "1. Embedding: Token IDs -> Vectors (wte)\n",
    "2. Stack of Blocks (Repeated L times):\n",
    "   - RMSNorm\n",
    "   - Attention (Mixing info between tokens)\n",
    "   - RMSNorm\n",
    "   - MLP (Processing info within a token)\n",
    "3. Final Norm \n",
    "4. LMHead: Vectors -> Logits (Probabilities)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Hyperparameters for the model.\n",
    "    \"\"\"\n",
    "    # ┌─────────────────────────────────────────────────────────┐\n",
    "    # │           321M CONVERSATIONAL MODEL                     │\n",
    "    # ├─────────────────────────────────────────────────────────┤\n",
    "    # │  hidden_dim:        1024                                │\n",
    "    # │  layers:            20                                  │\n",
    "    # │  heads:             8                                   │\n",
    "    # │  head_dim:          128                                 │\n",
    "    # │  mlp_ratio:         3x                                  │\n",
    "    # │  vocab_size:        32K                                 │\n",
    "    # │  context_length:    1024                                │\n",
    "    # │  embedding:         tied (input = output projection)    │\n",
    "    # │  activation:        relu squared                        │\n",
    "    # │  position encoding: RoPE                                │\n",
    "    # ├─────────────────────────────────────────────────────────┤\n",
    "    # │  TOTAL PARAMETERS:  243,269,632                         │\n",
    "    # └─────────────────────────────────────────────────────────┘\n",
    "    # No KV cache\n",
    "    # No GQA\n",
    "\n",
    "    hidden_dim: int = 1024 # hidden dimension\n",
    "    n_layers: int = 20 # May need to reduce to 22 or 20\n",
    "    n_heads: int = 8 # head dimension = hidden_dim / n_heads = 128\n",
    "    mlp_ratio: int = 3\n",
    "    vocab_size: int = 32*1024\n",
    "    sequence_len: int = 1024\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    RMSNorm (Root Mean Square Layer Normalization).\n",
    "    Used to stabilize training by normalizing activation magnitudes.\n",
    "    \"\"\"\n",
    "    # Purely functional rmsnorm with no learnable params\n",
    "    return F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "\n",
    "def apply_rotary_emb(x, cos, sin):\n",
    "    \"\"\"\n",
    "    Applies Rotary Positional Embeddings (RoPE).\n",
    "    Rotates the query and key vectors to encode relative positions.\n",
    "    \"\"\"\n",
    "    assert x.ndim == 4  # multihead attention\n",
    "    d = x.shape[3] // 2\n",
    "    x1, x2 = x[..., :d], x[..., d:] # split up last time into two halves\n",
    "    y1 = x1 * cos + x2 * sin # rotate pairs of dims\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    out = torch.cat([y1, y2], 3) # re-assemble\n",
    "    out = out.to(x.dtype) # ensure input/output dtypes match\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c550322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(hidden_dim=1024, n_layers=20, n_heads=8, mlp_ratio=3, vocab_size=32768, sequence_len=1024)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self Attention.\n",
    "    \n",
    "    1. Projects input to Q, K, V.\n",
    "    2. Applies RoPE to Q, K for position info.\n",
    "    3. Computes attention scores (Q @ K) to see how much each token cares about others. Aggregates values (V) based on scores.\n",
    "    4. Projects output to mix information across heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.head_dim = config.hidden_dim // config.n_heads\n",
    "\n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.key = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.query = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.value = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "\n",
    "        # Output projection (\"o\"): mixes results from all heads back into n_embd\n",
    "        self.proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cos_sin: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 1. Projects input to Q, K, V.\n",
    "        # reshape to (B, T, n_heads, head_dim)\n",
    "        k = self.key(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        q = self.query(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.value(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # 2. Applies RoPE to Q, K for position info.\n",
    "        cos, sin = cos_sin\n",
    "        k, q = apply_rotatory_positional_encoding(k, cos, sin), apply_rotatory_positional_encoding(q, cos, sin)\n",
    "\n",
    "        # 3. Computes attention scores (Q @ K) to see how much each token cares about others.\n",
    "        q, k = norm(q), norm(k) # QK norm\n",
    "\n",
    "        # make head be batch dim, i.e. (B, T, n_heads, head_dim) -> (B, n_heads, T, head_dim)\n",
    "        # We are making the n_heads into a batch dimension so pytorch treats it as batches and\n",
    "        # applies the attention function on each head separately in parallel\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)         \n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # Re-assemble the heads side by side and project back to residual stream\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 4. Projects output to mix information across heads.\n",
    "        y = self.proj(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21bcf98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = MultiHeadAttention(config)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ed627ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param in attn.parameters():\n",
    "    print(type(param), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Network (MLP).\n",
    "    Processes each token independently (no mixing between tokens).\n",
    "    Structure: Expand -> ReLU^2 -> Contract\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.proj_up = nn.Linear(config.hidden_dim, config.hidden_dim * config.mlp_ratio, bias=False)\n",
    "        self.proj_down = nn.Linear(config.hidden_dim * config.mlp_ratio, config.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj_up(x)\n",
    "        x = F.relu(x).square()\n",
    "        # TODO: Check if swiglu is better for 3 x hidden_dim - \n",
    "        # gelu and silu are alternatives but difference seems marginal so sticking with relu^2\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bebf83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (proj_up): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "  (proj_down): Linear(in_features=3072, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(config)\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00d63fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b83c5470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MultiHeadAttention                       [1, 1024, 1024]           --\n",
       "├─Linear: 1-1                            [1, 1024, 1024]           1,048,576\n",
       "├─Linear: 1-2                            [1, 1024, 1024]           1,048,576\n",
       "├─Linear: 1-3                            [1, 1024, 1024]           1,048,576\n",
       "├─Linear: 1-4                            [1, 1024, 1024]           1,048,576\n",
       "==========================================================================================\n",
       "Total params: 4,194,304\n",
       "Trainable params: 4,194,304\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.19\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 33.55\n",
       "Params size (MB): 16.78\n",
       "Estimated Total Size (MB): 54.53\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(attn, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6291466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FeedForward                              [1, 1024, 1024]           --\n",
       "├─Linear: 1-1                            [1, 1024, 3072]           3,145,728\n",
       "├─Linear: 1-2                            [1, 1024, 1024]           3,145,728\n",
       "==========================================================================================\n",
       "Total params: 6,291,456\n",
       "Trainable params: 6,291,456\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.29\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 33.55\n",
       "Params size (MB): 25.17\n",
       "Estimated Total Size (MB): 62.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ff, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eed103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Block.\n",
    "    Contains:\n",
    "    1. Attention (Communication)\n",
    "    2. MLP (Computation)\n",
    "    Both use Residual Connections (x + ...) and Pre-Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ff = FeedForward(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, cos_sin: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention with residual connection\n",
    "        x = x + self.attn(norm(x), cos_sin)\n",
    "        # MLP with residual connection\n",
    "        x = x + self.ff(norm(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c4ae3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (attn): MultiHeadAttention(\n",
       "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (proj_up): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "    (proj_down): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerBlock(GPTConfig())\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe299056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TransformerBlock                         [1, 1024, 1024]           --\n",
       "├─MultiHeadAttention: 1-1                [1, 1024, 1024]           --\n",
       "│    └─Linear: 2-1                       [1, 1024, 1024]           1,048,576\n",
       "│    └─Linear: 2-2                       [1, 1024, 1024]           1,048,576\n",
       "│    └─Linear: 2-3                       [1, 1024, 1024]           1,048,576\n",
       "│    └─Linear: 2-4                       [1, 1024, 1024]           1,048,576\n",
       "├─FeedForward: 1-2                       [1, 1024, 1024]           --\n",
       "│    └─Linear: 2-5                       [1, 1024, 3072]           3,145,728\n",
       "│    └─Linear: 2-6                       [1, 1024, 1024]           3,145,728\n",
       "==========================================================================================\n",
       "Total params: 10,485,760\n",
       "Trainable params: 10,485,760\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 10.49\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 67.11\n",
       "Params size (MB): 41.94\n",
       "Estimated Total Size (MB): 113.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(block, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    The full GPT model.\n",
    "    Contains:\n",
    "    1. Token Embedding\n",
    "    2. Transformer Blocks (stacked)\n",
    "    3. Final Normalization\n",
    "    4. LM Head - Tied weights with token embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "\n",
    "        self.rotary_seq_len = config.sequence_len * 10 # 10X over-compute\n",
    "        # Why 10x? This provides a generous buffer for inference/generation, allowing the model\n",
    "        # to generate sequences longer than its training length without recomputing embeddings.\n",
    "        # Note: While the embeddings support 10x length, the model's quality degrades beyond ~1.5-2x\n",
    "        # the training length due to unseen attention patterns. This buffer is for convenience,\n",
    "        # not an expectation of good performance at 10x length. Memory cost is negligible.\n",
    "        \n",
    "        head_dim = config.hidden_dim // config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False) # persistent=False means it's not saved to the checkpoint\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "    \n",
    "    def forward(self, idx, targets=None, loss_reduction=\"mean\") -> torch.Tensor:\n",
    "        T = idx.shape[1]\n",
    "        cos_sin = self.cos[:, :T], self.sin[:, :T] # truncate cache to current sequence length\n",
    "        x = self.token_embedding(x)\n",
    "        x = norm(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, cos_sin)\n",
    "        x = norm(x)\n",
    "\n",
    "        softcap = 15 # smoothly cap the logits to the range [-softcap, softcap]\n",
    "        logits = self.lm_head(x)\n",
    "        logits = logits.float() # switch to fp32 for logit softcap and loss computation\n",
    "        logits = softcap * torch.tanh(logits / softcap) # squash the logits\n",
    "        \n",
    "        if targets is not None:\n",
    "            # training: given the targets, compute and return the loss\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\n",
    "            return loss\n",
    "        else:\n",
    "            # inference: just return the logits directly\n",
    "            return logits\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Custom weight initialization scheme.\n",
    "        \n",
    "        The initialization logic deviates from PyTorch defaults (Kaiming defaults) to improve training \n",
    "        stability and convergence for deep Transformers.\n",
    "        \n",
    "        Key Differences:\n",
    "        1. Zero Initialization for Output Projections:\n",
    "           - Function: Sets the weights of the final linear layer in each block to zero.\n",
    "           - Why: This ensures that at initialization, the residual blocks contribute nothing to the \n",
    "             residual stream (y = x + 0). The model effectively starts as an identity function, allowing \n",
    "             unimpeded gradient flow from top to bottom. This prevents vanishing/exploding gradients \n",
    "             and provides a stable starting point for the model to gradually learn features.\n",
    "\n",
    "        2. Zero Initialization for LM Head:\n",
    "           - Function: Sets the classifier weights to zero.\n",
    "           - Why: Ensures all logits are initially zero, leading to a uniform probability distribution (1/V) \n",
    "             for the next token. This minimizes the initial loss to exactly log(V) and prevents the model \n",
    "             from starting with random biases towards arbitrary tokens.\n",
    "        \"\"\"\n",
    "        # Initialize the weights of the model\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Zero out the output projections of the blocks\n",
    "        for block in self.blocks:\n",
    "            torch.nn.init.zeros_(block.ff.proj_down.weight)\n",
    "            torch.nn.init.zeros_(block.attn.proj.weight)\n",
    "\n",
    "        # init the rotary embeddings\n",
    "        head_dim = self.config.n_embd // self.config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.cos, self.sin = cos, sin\n",
    "\n",
    "        # Cast the embeddings from fp32 to bf16: optim can tolerate it and it saves memory: both in the model and the activations\n",
    "        if self.token_embedding.weight.device.type == \"cuda\":\n",
    "            self.token_embedding.to(dtype=torch.bfloat16)\n",
    "\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Custom initialization for Linear and Embedding layers.\n",
    "        \n",
    "        1. Controlled Variance (Linear Layers):\n",
    "           - Formula: std = 1 / sqrt(fan_in) * min(1, sqrt(fan_out / fan_in))\n",
    "           - Why: Standard Kaiming init often leads to activation variance that grows with depth in \n",
    "             Transformers. This custom initialization (ref: https://arxiv.org/pdf/2310.17813) stabilizes \n",
    "             activation variance across layers, specifically accounting for the network width.\n",
    "\n",
    "        2. Unit Variance (Embeddings):\n",
    "           - Function: Normal distribution with std=1.0.\n",
    "           - Why: Ensures strong initial signal strength before it enters the first normalization layer.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # https://arxiv.org/pdf/2310.17813 \n",
    "            fan_out = module.weight.size(0)\n",
    "            fan_in = module.weight.size(1)\n",
    "            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
    "\n",
    "\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n",
    "        # autodetect the device from model embeddings\n",
    "        if device is None:\n",
    "            device = self.token_embedding.weight.device\n",
    "        # stride the channels\n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "        # stride the time steps\n",
    "        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "        # calculate the rotation frequencies at each (time, channel) pair\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        cos, sin = freqs.cos(), freqs.sin()\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16() # keep them in bfloat16\n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :] # add batch and head dims for later broadcasting\n",
    "        return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb5f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6f25595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GPT                                      [1, 1024, 32768]          --\n",
       "├─Embedding: 1-1                         [1, 1024, 1024]           33,554,432\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-1      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-2             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-2             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-3      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-4             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-3             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-5      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-6             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-4             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-7      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-8             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-5             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-9      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-10            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-6             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-11     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-12            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-7             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-13     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-14            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-8             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-15     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-16            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-9             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-17     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-18            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-10            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-19     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-20            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-11            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-21     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-22            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-12            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-23     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-24            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-13            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-25     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-26            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-14            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-27     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-28            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-15            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-29     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-30            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-16            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-31     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-32            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-17            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-33     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-34            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-18            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-35     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-36            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-19            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-37     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-38            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-20            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-39     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-40            [1, 1024, 1024]           6,291,456\n",
       "├─Linear: 1-3                            [1, 1024, 32768]          33,554,432\n",
       "==========================================================================================\n",
       "Total params: 276,824,064\n",
       "Trainable params: 276,824,064\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 276.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1619.00\n",
       "Params size (MB): 1107.30\n",
       "Estimated Total Size (MB): 2726.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(gpt, input_size=(1, config.sequence_len), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47fa1992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual unique parameters: 243,269,632\n"
     ]
    }
   ],
   "source": [
    "# The weight tying is working correctly — torchinfo just doesn't detect shared parameters by default. \n",
    "# It counts each layer's parameters independently.\n",
    "\n",
    "# This counts UNIQUE parameters (correct count with tying)\n",
    "real_params = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"Actual unique parameters: {real_params:,}\")\n",
    "\n",
    "# 318M - 32M (duplicate embedding) ≈ 286M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "993fe345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object: True\n",
      "Same memory: True\n"
     ]
    }
   ],
   "source": [
    "# These should all be True\n",
    "print(\"Same object:\", gpt.lm_head.weight is gpt.token_embedding.weight)\n",
    "print(\"Same memory:\", gpt.lm_head.weight.data_ptr() == gpt.token_embedding.weight.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb255ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67bd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19db082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture Overview:\n",
    "1. Embedding: Token IDs -> Vectors (wte)\n",
    "2. Stack of Blocks (Repeated L times):\n",
    "   - RMSNorm\n",
    "   - Attention (Mixing info between tokens)\n",
    "   - RMSNorm\n",
    "   - MLP (Processing info within a token)\n",
    "3. Final Norm \n",
    "4. LMHead: Vectors -> Logits (Probabilities)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Hyperparameters for the model.\n",
    "    \"\"\"\n",
    "    # ┌─────────────────────────────────────────────────────────┐\n",
    "    # │           321M CONVERSATIONAL MODEL                     │\n",
    "    # ├─────────────────────────────────────────────────────────┤\n",
    "    # │  hidden_dim:        1024                                │\n",
    "    # │  layers:            20                                  │\n",
    "    # │  heads:             8                                   │\n",
    "    # │  head_dim:          128                                 │\n",
    "    # │  mlp_ratio:         3x                                  │\n",
    "    # │  vocab_size:        32K                                 │\n",
    "    # │  context_length:    1024                                │\n",
    "    # │  embedding:         tied (input = output projection)    │\n",
    "    # │  activation:        relu squared                        │\n",
    "    # │  position encoding: RoPE                                │\n",
    "    # ├─────────────────────────────────────────────────────────┤\n",
    "    # │  TOTAL PARAMETERS:  243,269,632                        │\n",
    "    # └─────────────────────────────────────────────────────────┘\n",
    "    # No KV cache\n",
    "    # No GQA\n",
    "\n",
    "    hidden_dim: int = 1024 # hidden dimension\n",
    "    n_layers: int = 20 # May need to reduce to 22 or 20\n",
    "    n_heads: int = 8 # head dimension = hidden_dim / n_heads = 128\n",
    "    mlp_ratio: int = 3\n",
    "    vocab_size: int = 32*1024\n",
    "    sequence_len: int = 1024\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    RMSNorm (Root Mean Square Layer Normalization).\n",
    "    Used to stabilize training by normalizing activation magnitudes.\n",
    "    \"\"\"\n",
    "    # Purely functional rmsnorm with no learnable params\n",
    "    return F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "\n",
    "# def apply_rotatory_positional_encoding(x: torch.Tensor, head_dim: int) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Apply RoPE to the input tensor.\n",
    "#     \"\"\"\n",
    "#     B, T, C = x.size()\n",
    "#     # TODO: Implement RoPE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c550322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(hidden_dim=1024, n_layers=20, n_heads=8, mlp_ratio=3, vocab_size=32768, sequence_len=1024)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2883e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self Attention.\n",
    "    \n",
    "    1. Projects input to Q, K, V.\n",
    "    2. Applies RoPE to Q, K for position info.\n",
    "    3. Computes attention scores (Q @ K) to see how much each token cares about others. Aggregates values (V) based on scores.\n",
    "    4. Projects output to mix information across heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.head_dim = config.hidden_dim // config.n_heads\n",
    "\n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.key = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.query = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.value = nn.Linear(self.hidden_dim, self.head_dim * self.n_heads, bias=False)\n",
    "\n",
    "        # Output projection (\"o\"): mixes results from all heads back into n_embd\n",
    "        self.proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 1. Projects input to Q, K, V.\n",
    "        k = self.key(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        q = self.query(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.value(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # 2. Applies RoPE to Q, K for position info.\n",
    "        # TODO: Implement RoPE\n",
    "        # k = apply_rotatory_positional_encoding(k, self.head_dim)\n",
    "        # q = apply_rotatory_positional_encoding(q, self.head_dim)\n",
    "\n",
    "        # 3. Computes attention scores (Q @ K) to see how much each token cares about others.\n",
    "        q, k = norm(q), norm(k) # QK norm\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) # make head be batch dim, i.e. (B, T, H, D) -> (B, H, T, D)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # Re-assemble the heads side by side and project back to residual stream\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 4. Projects output to mix information across heads.\n",
    "        y = self.proj(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21bcf98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = MultiHeadAttention(config)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ed627ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for param in attn.parameters():\n",
    "    print(type(param), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc2fc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Network (MLP).\n",
    "    Processes each token independently (no mixing between tokens).\n",
    "    Structure: Expand -> ReLU^2 -> Contract\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.proj_up = nn.Linear(config.hidden_dim, config.hidden_dim * config.mlp_ratio, bias=False)\n",
    "        self.proj_down = nn.Linear(config.hidden_dim * config.mlp_ratio, config.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj_up(x)\n",
    "        x = F.relu(x).square()\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bebf83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (proj_up): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "  (proj_down): Linear(in_features=3072, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(config)\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00d63fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b83c5470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MultiHeadAttention                       [1, 1024, 1024]           --\n",
       "├─Linear: 1-1                            [1, 1024, 1024]           1,048,576\n",
       "├─Linear: 1-2                            [1, 1024, 1024]           1,048,576\n",
       "├─Linear: 1-3                            [1, 1024, 1024]           1,048,576\n",
       "├─Linear: 1-4                            [1, 1024, 1024]           1,048,576\n",
       "==========================================================================================\n",
       "Total params: 4,194,304\n",
       "Trainable params: 4,194,304\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.19\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 33.55\n",
       "Params size (MB): 16.78\n",
       "Estimated Total Size (MB): 54.53\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(attn, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6291466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FeedForward                              [1, 1024, 1024]           --\n",
       "├─Linear: 1-1                            [1, 1024, 3072]           3,145,728\n",
       "├─Linear: 1-2                            [1, 1024, 1024]           3,145,728\n",
       "==========================================================================================\n",
       "Total params: 6,291,456\n",
       "Trainable params: 6,291,456\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.29\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 33.55\n",
       "Params size (MB): 25.17\n",
       "Estimated Total Size (MB): 62.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ff, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4eed103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Block.\n",
    "    Contains:\n",
    "    1. Attention (Communication)\n",
    "    2. MLP (Computation)\n",
    "    Both use Residual Connections (x + ...) and Pre-Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ff = FeedForward(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention with residual connection\n",
    "        x = self.attn(norm(x)) + x\n",
    "        # MLP with residual connection\n",
    "        x = self.ff(norm(x)) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c4ae3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (attn): MultiHeadAttention(\n",
       "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (proj_up): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "    (proj_down): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerBlock(GPTConfig())\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe299056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TransformerBlock                         [1, 1024, 1024]           --\n",
       "├─MultiHeadAttention: 1-1                [1, 1024, 1024]           --\n",
       "│    └─Linear: 2-1                       [1, 1024, 1024]           1,048,576\n",
       "│    └─Linear: 2-2                       [1, 1024, 1024]           1,048,576\n",
       "│    └─Linear: 2-3                       [1, 1024, 1024]           1,048,576\n",
       "│    └─Linear: 2-4                       [1, 1024, 1024]           1,048,576\n",
       "├─FeedForward: 1-2                       [1, 1024, 1024]           --\n",
       "│    └─Linear: 2-5                       [1, 1024, 3072]           3,145,728\n",
       "│    └─Linear: 2-6                       [1, 1024, 1024]           3,145,728\n",
       "==========================================================================================\n",
       "Total params: 10,485,760\n",
       "Trainable params: 10,485,760\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 10.49\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 67.11\n",
       "Params size (MB): 41.94\n",
       "Estimated Total Size (MB): 113.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(block, input_size=(1, config.sequence_len, config.hidden_dim), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "694a4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    The full GPT model.\n",
    "    Contains:\n",
    "    1. Token Embedding\n",
    "    2. Transformer Blocks (stacked)\n",
    "    3. Final Normalization\n",
    "    4. Output Head (LM Head). Same as input embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        # TODO: check how to tie the weights        \n",
    "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.token_embedding(x)\n",
    "        x = norm(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        # logits = softcap * torch.tanh(logits / softcap) # squash the logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb5f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6f25595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GPT                                      [1, 1024, 32768]          --\n",
       "├─Embedding: 1-1                         [1, 1024, 1024]           33,554,432\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-1      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-2             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-2             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-3      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-4             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-3             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-5      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-6             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-4             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-7      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-8             [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-5             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-9      [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-10            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-6             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-11     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-12            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-7             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-13     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-14            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-8             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-15     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-16            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-9             [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-17     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-18            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-10            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-19     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-20            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-11            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-21     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-22            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-12            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-23     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-24            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-13            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-25     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-26            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-14            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-27     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-28            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-15            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-29     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-30            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-16            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-31     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-32            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-17            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-33     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-34            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-18            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-35     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-36            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-19            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-37     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-38            [1, 1024, 1024]           6,291,456\n",
       "│    └─TransformerBlock: 2-20            [1, 1024, 1024]           --\n",
       "│    │    └─MultiHeadAttention: 3-39     [1, 1024, 1024]           4,194,304\n",
       "│    │    └─FeedForward: 3-40            [1, 1024, 1024]           6,291,456\n",
       "├─Linear: 1-3                            [1, 1024, 32768]          33,554,432\n",
       "==========================================================================================\n",
       "Total params: 276,824,064\n",
       "Trainable params: 276,824,064\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 276.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1619.00\n",
       "Params size (MB): 1107.30\n",
       "Estimated Total Size (MB): 2726.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(gpt, input_size=(1, config.sequence_len), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47fa1992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual unique parameters: 243,269,632\n"
     ]
    }
   ],
   "source": [
    "# The weight tying is working correctly — torchinfo just doesn't detect shared parameters by default. \n",
    "# It counts each layer's parameters independently.\n",
    "\n",
    "# This counts UNIQUE parameters (correct count with tying)\n",
    "real_params = sum(p.numel() for p in gpt.parameters())\n",
    "print(f\"Actual unique parameters: {real_params:,}\")\n",
    "\n",
    "# 318M - 32M (duplicate embedding) ≈ 286M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "993fe345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object: True\n",
      "Same memory: True\n"
     ]
    }
   ],
   "source": [
    "# These should all be True\n",
    "print(\"Same object:\", gpt.lm_head.weight is gpt.token_embedding.weight)\n",
    "print(\"Same memory:\", gpt.lm_head.weight.data_ptr() == gpt.token_embedding.weight.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb255ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67bd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
